# mnist-deep-cnn-improved-image-classification

ğŸ“˜ Project Overview

This repository presents a research-driven exploration into improving the performance and generalization of Convolutional Neural Networks (CNNs) on the MNIST handwritten digit dataset.
We begin with a baseline 2-block CNN and progressively enhance it using:

Batch Normalization

Dropout Regularization

Data Augmentation

Improved training dynamics

The result is a deeper, more stable, and more robust CNN architecture capable of achieving state-of-the-art MNIST performance with strong generalization characteristics.

This repository provides both the baseline and improved models for comparison, enabling reproducible benchmarking and architecture experimentation.

ğŸ¯ Research Objectives

This project investigates:

1. How classic CNNs behave on MNIST using minimal architecture.

Baseline model uses only Conv â†’ Pool â†’ Conv â†’ Pool â†’ Dense.

2. How to systematically improve model robustness and generalization using:

Batch normalization

Dropout at multiple depths

Data augmentation (rotation, translation, zoom)

Improved regularization strategies

3. Convergence stability & learning dynamics

Training vs validation curves

Loss behaviors

Trainâ€“validation accuracy gap

Sensitivity to augmentation

4. Generalization to shifted, rotated, or deformed digits

Evaluating how augmentation affects robustness.

ğŸ“Š Key Results
Baseline CNN

Accuracy: ~99% test accuracy

Behavior: Fast convergence, no overfitting

Weakness: Less robust to shifts & rotations

Improved CNN

Accuracy: Higher-than-baseline

Techniques used:
âœ“ Batch Normalization
âœ“ Dropout (0.25/0.25/0.5)
âœ“ Data Augmentation

Behavior:

Smoother optimization

Higher generalization

More stable validation loss

Stronger performance on perturbed digits

ğŸ§  Architecture Summary
Baseline Architecture
Input â†’ Conv(32) â†’ MaxPool â†’
        Conv(64) â†’ MaxPool â†’
        Flatten â†’ Dense(128) â†’ Dense(10)

Improved Architecture
Input â†’ Data Augmentation â†’
    Conv(32) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.25)
    Conv(64) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout(0.25)
    Flatten â†’
    Dense(128) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5) â†’
    Dense(10, softmax)

ğŸ“¦ Repository Structure

A clean, research-style folder layout:

mnist-deep-cnn-improved-image-classification/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_baseline_cnn.ipynb
â”‚   â”œâ”€â”€ 02_improved_cnn.ipynb
â”‚   â””â”€â”€ 03_results_analysis.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ baseline_model.py
â”‚   â”œâ”€â”€ improved_model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/
â”‚   â”‚   â”œâ”€â”€ baseline_accuracy_curve.png
â”‚   â”‚   â”œâ”€â”€ improved_accuracy_curve.png
â”‚   â”‚   â””â”€â”€ comparison.png
â”‚   â””â”€â”€ metrics/
â”‚       â”œâ”€â”€ baseline_metrics.json
â”‚       â””â”€â”€ improved_metrics.json
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸš€ How to Run
Clone repo
git clone https://github.com/obiedeh/mnist-deep-cnn-improved-image-classification.git
cd mnist-deep-cnn-improved-image-classification

Install dependencies
pip install -r requirements.txt

Train baseline model
python src/train.py --model baseline

Train improved model
python src/train.py --model improved

ğŸ“ˆ Training Visualizations

The repository includes comparison plots:

Baseline vs Improved accuracy curves

Baseline vs Improved loss curves

Prediction samples with confidence

Generalization performance improvements

These are generated automatically inside the results/plots/ folder.

ğŸ“ Research Notes & Insights
Findings

Batch Normalization improves optimization stability.

Dropout prevents co-adaptation of layers.

Augmentation significantly increases robustness to digit variability.

Improved CNN consistently outperforms baseline on rotated/shifted digits.

Baseline CNN already performs well, but improvement techniques make the model deploy-ready.

Why This Matters

MNIST is often considered â€œtoo easy,â€ but it provides the perfect controlled environment for studying:

Learning dynamics

Regularization effectiveness

Convergence patterns

Model robustness

This project demonstrates how classic CNNs can be turned from high-performing into highly robust.

ğŸ“š Technologies Used

TensorFlow / Keras

NumPy

Matplotlib

Python 3.10+

ğŸ§ª Sample Research Questions the Repo Addresses

How much does data augmentation influence CNN generalization?

Does dropout at early layers or late layers matter more?

What role does batch normalization play in convergence?

How close can a small CNN get to â€œstate-of-the-artâ€?

How do baseline and improved models differ in learning dynamics?

ğŸ¤ Contributions

Pull requests for additional architectures (e.g., ResNet-style MNIST, depthwise CNNs, transformer baselines) are welcome.

ğŸ“„ License

MIT License.
