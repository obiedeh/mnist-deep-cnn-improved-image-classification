# -*- coding: utf-8 -*-
"""mnist-cnn-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b127TFCGtg08xtRhwfHdBnakxnJerTc_
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Install TensorFlow if needed (uncomment in a notebook / Colab)
# !pip install tensorflow

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# ============================================================
# MNIST CNN Classification for Hand-Written Digits (0-9)
# Baseline CNN + Improved CNN (BN + Dropout + Augmentation)
# ============================================================

# ------------------------------------------------------------
# Step 1: Import MNIST Dataset and Re-scale
# ------------------------------------------------------------

# Load MNIST dataset (has pre-defined train/test split)
(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()

# Re-scale images to [0, 1] range
x_train_mnist = x_train_mnist.astype("float32") / 255.0
x_test_mnist = x_test_mnist.astype("float32") / 255.0

# Add channel dimension for CNN (grayscale = 1 channel)
# Shape: (samples, height, width) -> (samples, height, width, channels)
x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)
x_test_mnist = x_test_mnist.reshape(-1, 28, 28, 1)

print(f"Training data shape: {x_train_mnist.shape}")
print(f"Test data shape: {x_test_mnist.shape}")
print(f"Number of training samples: {len(x_train_mnist)}")
print(f"Number of test samples: {len(x_test_mnist)}")
print(f"Image pixel value range: [{x_train_mnist.min()}, {x_train_mnist.max()}]")

# ------------------------------------------------------------
# Step 2: Baseline CNN with Two Convolution-Pool Blocks
# ------------------------------------------------------------

# Build CNN model using Functional API
inputs_cnn = keras.Input(shape=(28, 28, 1))

# First convolution-pool block
x = layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs_cnn)
x = layers.MaxPooling2D(pool_size=(2, 2))(x)

# Second convolution-pool block
x = layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)
x = layers.MaxPooling2D(pool_size=(2, 2))(x)

# Flatten for classifier
x = layers.Flatten()(x)

# Classifier (dense layers)
x = layers.Dense(128, activation='relu')(x)
outputs_cnn = layers.Dense(10, activation='softmax')(x)  # 10 digits (0-9)

# Create baseline model
cnn_model = keras.Model(inputs=inputs_cnn, outputs=outputs_cnn, name='mnist_cnn_baseline')

# Display model architecture
cnn_model.summary()

# ------------------------------------------------------------
# Step 3: Compile and Train the Baseline Model
# ------------------------------------------------------------

cnn_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_cnn = cnn_model.fit(
    x_train_mnist,
    y_train_mnist,
    epochs=5,
    batch_size=64,
    validation_split=0.1,
    verbose=1
)

# ------------------------------------------------------------
# Step 4: Plot Learning Curves for Baseline Model
# ------------------------------------------------------------

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history_cnn.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history_cnn.history['val_accuracy'], label='Validation Accuracy', marker='s')
plt.title('Baseline Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history_cnn.history['loss'], label='Training Loss', marker='o')
plt.plot(history_cnn.history['val_loss'], label='Validation Loss', marker='s')
plt.title('Baseline Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Print final metrics
print(f"\nFinal Training Accuracy (Baseline): {history_cnn.history['accuracy'][-1]:.4f}")
print(f"Final Validation Accuracy (Baseline): {history_cnn.history['val_accuracy'][-1]:.4f}")
print(f"Final Training Loss (Baseline): {history_cnn.history['loss'][-1]:.4f}")
print(f"Final Validation Loss (Baseline): {history_cnn.history['val_loss'][-1]:.4f}")

# ------------------------------------------------------------
# Step 5: Make Predictions on Sample Images (Baseline Model)
# ------------------------------------------------------------

np.random.seed(42)
sample_indices = np.random.choice(len(x_test_mnist), 3, replace=False)

sample_images = x_test_mnist[sample_indices]
sample_labels = y_test_mnist[sample_indices]

predictions = cnn_model.predict(sample_images)
predicted_classes = np.argmax(predictions, axis=1)

plt.figure(figsize=(12, 4))
for i in range(3):
    plt.subplot(1, 3, i + 1)
    plt.imshow(sample_images[i].reshape(28, 28), cmap='gray')
    plt.title(
        f"True: {sample_labels[i]}, Pred: {predicted_classes[i]}\n"
        f"Conf: {predictions[i][predicted_classes[i]]:.2%}"
    )
    plt.axis('off')
plt.tight_layout()
plt.show()

print("\nPrediction Details (Baseline Model):")
for i in range(3):
    print(f"\nSample {i+1}:")
    print(f"  True Label: {sample_labels[i]}")
    print(f"  Predicted Label: {predicted_classes[i]}")
    print(f"  Confidence: {predictions[i][predicted_classes[i]]:.2%}")
    print(f"  Correct: {'✓' if predicted_classes[i] == sample_labels[i] else '✗'}")

# ------------------------------------------------------------
# Step 6: Evaluate Baseline Model Convergence
# ------------------------------------------------------------

test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(x_test_mnist, y_test_mnist, verbose=0)

print("=" * 60)
print("BASELINE MODEL CONVERGENCE ANALYSIS")
print("=" * 60)
print(f"\nTest Accuracy: {test_accuracy_cnn:.4f}")
print(f"Test Loss: {test_loss_cnn:.4f}")

training_acc = history_cnn.history['accuracy']
val_acc = history_cnn.history['val_accuracy']
training_loss = history_cnn.history['loss']
val_loss = history_cnn.history['val_loss']

print("\n" + "-" * 60)
print("CONVERGENCE INDICATORS (BASELINE):")
print("-" * 60)

# Check if accuracy is improving
if training_acc[-1] > training_acc[0]:
    print("✓ Training accuracy improved from {:.2%} to {:.2%}".format(training_acc[0], training_acc[-1]))
else:
    print("✗ Training accuracy did not improve")

# Check if validation accuracy is improving
if val_acc[-1] > val_acc[0]:
    print("✓ Validation accuracy improved from {:.2%} to {:.2%}".format(val_acc[0], val_acc[-1]))
else:
    print("✗ Validation accuracy did not improve")

# Check for overfitting
acc_gap = training_acc[-1] - val_acc[-1]
if acc_gap < 0.05:
    print(f"✓ Small gap between training and validation accuracy ({acc_gap:.2%}) - no significant overfitting")
elif acc_gap < 0.10:
    print(f"⚠ Moderate gap between training and validation accuracy ({acc_gap:.2%}) - slight overfitting")
else:
    print(f"✗ Large gap between training and validation accuracy ({acc_gap:.2%}) - overfitting detected")

# Check loss convergence (FIXED: print final val_loss, not index 1)
if val_loss[-1] < val_loss[0]:
    print(f"✓ Validation loss decreased from {val_loss[0]:.4f} to {val_loss[-1]:.4f}")
else:
    print(f"✗ Validation loss did not decrease")

print("\n" + "=" * 60)
print("BASELINE MODEL CONCLUSION:")
print("=" * 60)
if test_accuracy_cnn > 0.97 and acc_gap < 0.05:
    print("✓ Model converged appropriately with high accuracy and no overfitting")
elif test_accuracy_cnn > 0.95:
    print("⚠ Model converged with good accuracy but could be improved")
else:
    print("✗ Model needs more training or architectural improvements")
print("=" * 60)

# ============================================================
# Step 7: Improved Model - Dropout, BatchNorm, Data Augmentation
# ============================================================

# Data Augmentation pipeline
data_augmentation = keras.Sequential(
    [
        layers.RandomRotation(0.1),
        layers.RandomTranslation(0.1, 0.1),
        layers.RandomZoom(0.1),
    ],
    name="data_augmentation",
)

# Build improved CNN model with BatchNorm, Dropout, and Data Augmentation
inputs_cnn_improved = keras.Input(shape=(28, 28, 1))

# Apply data augmentation only during training
y = data_augmentation(inputs_cnn_improved)

# First convolutional block
y = layers.Conv2D(32, (3, 3), padding="same", use_bias=False)(y)
y = layers.BatchNormalization()(y)
y = layers.Activation("relu")(y)
y = layers.MaxPooling2D((2, 2))(y)
y = layers.Dropout(0.25)(y)   # Dropout after first block

# Second convolutional block
y = layers.Conv2D(64, (3, 3), padding="same", use_bias=False)(y)
y = layers.BatchNormalization()(y)
y = layers.Activation("relu")(y)
y = layers.MaxPooling2D((2, 2))(y)
y = layers.Dropout(0.25)(y)   # Dropout after second block

# Classifier head
y = layers.Flatten()(y)
y = layers.Dense(128, use_bias=False)(y)
y = layers.BatchNormalization()(y)
y = layers.Activation("relu")(y)
y = layers.Dropout(0.5)(y)    # Stronger dropout before final layer
outputs_cnn_improved = layers.Dense(10, activation="softmax")(y)

cnn_model_improved = keras.Model(
    inputs=inputs_cnn_improved,
    outputs=outputs_cnn_improved,
    name="mnist_cnn_improved",
)

cnn_model_improved.summary()

# Compile improved model
cnn_model_improved.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"],
)

# Train improved model
history_cnn_improved = cnn_model_improved.fit(
    x_train_mnist,
    y_train_mnist,
    epochs=5,          # can increase to 10 for more benefit
    batch_size=64,
    validation_split=0.1,
    verbose=1,
)

# ------------------------------------------------------------
# Plot Learning Curves for Improved Model
# ------------------------------------------------------------

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history_cnn_improved.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history_cnn_improved.history['val_accuracy'], label='Validation Accuracy', marker='s')
plt.title('Improved Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history_cnn_improved.history['loss'], label='Training Loss', marker='o')
plt.plot(history_cnn_improved.history['val_loss'], label='Validation Loss', marker='s')
plt.title('Improved Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# Evaluate Improved Model and Compare
# ------------------------------------------------------------

test_loss_imp, test_accuracy_imp = cnn_model_improved.evaluate(x_test_mnist, y_test_mnist, verbose=0)
print("\n" + "=" * 60)
print("IMPROVED MODEL EVALUATION")
print("=" * 60)
print(f"Improved Model - Test Accuracy: {test_accuracy_imp:.4f}")
print(f"Improved Model - Test Loss: {test_loss_imp:.4f}")

print("\n" + "=" * 60)
print("BASELINE vs IMPROVED (TEST SET):")
print("=" * 60)
print(f"Baseline Test Accuracy: {test_accuracy_cnn:.4f}")
print(f"Improved  Test Accuracy: {test_accuracy_imp:.4f}")
print("=" * 60)